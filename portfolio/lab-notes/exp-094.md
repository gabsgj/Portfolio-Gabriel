# Generative Adversarial Nets (EXP-094)

**Status:** Training  
**Epoch:** 402  
**Started:** 2023-08-15

## Hypothesis

Small-scale GANs can achieve 90%+ perceptual fidelity on edge devices using quantized weights and optimized architectures.

## Background

Most style transfer solutions rely on large pretrained models (50-200MB) that are impractical for real-time edge deployment. This experiment explores whether aggressive quantization and architecture pruning can maintain visual quality.

## What Worked

- **Quantized weights (INT8)**: Reduced model size from 180MB to 42MB with only 3% quality degradation
- **Reduced discriminator depth**: Removed 2 conv layers, training remained stable
- **Progressive resizing**: Start at 64x64, scale to 256x256 during training
- **Spectral normalization**: Essential for stability at lower precision

## What Failed

- **BatchNorm instability**: INT8 BatchNorm parameters caused mode collapse at epoch 180. Switched to InstanceNorm.
- **Aggressive pruning (>70%)**: Quality dropped significantly below 80% prune ratio
- **Direct INT4 quantization**: Artifacts were too severe, abandoned

## Current Results

| Metric | Target | Current |
|--------|--------|---------|
| FID Score | <50 | 47.3 |
| Model Size | <50MB | 42MB |
| Inference (mobile) | <100ms | 89ms |
| Perceptual Quality | 90% | 87% |

## Next Steps

1. Experiment with knowledge distillation from larger teacher model
2. Test on Raspberry Pi 4 for embedded deployment
3. Try mixed-precision (INT8 backbone, FP16 final layers)

## Code Snippet

```python
class LightweightGenerator(nn.Module):
    def __init__(self, nz=100, ngf=64):
        super().__init__()
        self.main = nn.Sequential(
            # Input: nz x 1 x 1
            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),
            nn.InstanceNorm2d(ngf * 8),
            nn.ReLU(True),
            # ... simplified architecture
        )
```

---

*Last updated: 2023-11-20*
